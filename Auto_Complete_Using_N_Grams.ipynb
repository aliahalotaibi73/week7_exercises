{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliahalotaibi73/week7_exercises/blob/main/Auto_Complete_Using_N_Grams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE! Change run time type to T4 GPU inorder to speed up the process time.**"
      ],
      "metadata": {
        "id": "Ig1dTbzPHeIm"
      }
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'tweets-blogs-news-swiftkey-dataset-4million:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F6261%2F9186%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240901%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240901T142205Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D36ffc720428430960a7d0b7a44b0ff2d6011bbb1e559da0fc3ecd1f29b69e4b0c729d6d7195a8ddf46b1db9ffc1a701f670926e9a417b2050b132bd52a6e6355f672ae04512185c27440464778334ddc11bee13e87b540505340bdebcea015471343d86bb6a15f3367ea2a866bffda3cb07879c12bbc9811260dae8c3974868bd24bb687f78c4b8d9c467a901e7450f36a5a44b7d1e474a36cc5b5e291b69cc754955317e472b5a145445892d25e5a83d62cc1d5b0b4b8cb6e9a97f062dd542accc102bde8dfc0336556dae263987cc797752e99f94fa32a7d9320d9f8155c3a0680c6d5310fe3b5f7fdd2ebc195def4ac74e80e6ff1f1389c76d50a7a605e84'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "trUtaA-99vU6",
        "outputId": "f80a0399-3292-436f-e9d5-ab4c14baf0aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading tweets-blogs-news-swiftkey-dataset-4million, 1180692218 bytes compressed\n",
            "[==================================================] 1180692218 bytes downloaded\n",
            "Downloaded and uncompressed: tweets-blogs-news-swiftkey-dataset-4million\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "loV-iP5I9vVB"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"basic-setup\"></a>\n",
        "# üìÇ Basic Setup"
      ]
    },
    {
      "metadata": {
        "id": "kYncFEoI9vVD"
      },
      "cell_type": "markdown",
      "source": [
        "In this **hidden** code cell, we'll import our packages. As we'll implement n-gram models from scratch we'll just use numpy and Additionally nltk  (just for Tokenization)."
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "m7egLAtY9vVD"
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "## Importing Packages\n",
        "import math\n",
        "import nltk\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Basic File Paths\n",
        "data_dir = \"../input/tweets-blogs-news-swiftkey-dataset-4million/final/en_US\"\n",
        "file_path = data_dir + \"/en_US.twitter.txt\"\n",
        "\n",
        "## nltk settings\n",
        "nltk.data.path.append(data_dir)\n",
        "nltk.download('punkt')\n",
        "\n",
        "## Opening the File in read mode (\"r\")\n",
        "with open(file_path, \"r\") as f:\n",
        "    data = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tAYi2THj9vVD"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"pre-process\"></a>\n",
        "# üßΩ Pre-Processing pipeline (Note: As our dataset is quite big, this process will take up to 10 min)"
      ]
    },
    {
      "metadata": {
        "id": "MB_QcJ8d9vVE"
      },
      "cell_type": "markdown",
      "source": [
        "We create a simple pipeline function which:\n",
        "\n",
        "* splits the datasets by the `\\n` character\n",
        "\n",
        "* remove leading and trailing spaces\n",
        "\n",
        "* drop empty sentences.\n",
        "\n",
        "* Tokenize sentences using `nltk.word_tokenize`"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ADmvu0C09vVF"
      },
      "cell_type": "code",
      "source": [
        "def preprocess_pipeline(data) -> 'list':\n",
        "\n",
        "    # Split by newline character\n",
        "    sentences = data.split('\\n')\n",
        "\n",
        "    # Remove leading and trailing spaces\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "\n",
        "    # Drop Empty Sentences\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "\n",
        "    # Empty List to hold Tokenized Sentences\n",
        "    tokenized = []\n",
        "\n",
        "    # Iterate through sentences\n",
        "    for sentence in sentences:\n",
        "\n",
        "        # Convert to lowercase\n",
        "        sentence = sentence.lower()\n",
        "\n",
        "        # Convert to a list of words\n",
        "        token = nltk.word_tokenize(sentence)\n",
        "\n",
        "        # Append to list\n",
        "        tokenized.append(token)\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "## Pass our data to this function\n",
        "tokenized_sentences = preprocess_pipeline(data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_pipeline(data) -> 'list':\n",
        "\n",
        "    sentences = data.split('\\n')\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    sentences = [s for s in sentences if len(s)] > 0\n",
        "\n",
        "    tokenized = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "      sentence = sentences.lower()\n",
        "\n",
        "      token = nltk.word_tokenize(sentence)\n",
        "      tokenized.append(token)\n",
        "\n",
        "      return tokenized\n",
        "\n",
        "\n",
        "tokenized_sentences = preprocess_pipeline(data)\n"
      ],
      "metadata": {
        "id": "D6uEjKnj7iqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8YxS-dYa9vVH"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"split\"></a>\n",
        "# ‚úÇÔ∏è Splitting into Train, Valid and Test"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "dhhHIUPQ9vVH"
      },
      "cell_type": "code",
      "source": [
        "## Obtain Train and Test Split\n",
        "train, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "## Obtain Train and Validation Split\n",
        "train, val = train_test_split(train, test_size=0.25, random_state=42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILf3vi7q9vVJ"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"clean\"></a>\n",
        "# üßπ Cleaning the Data"
      ]
    },
    {
      "metadata": {
        "id": "d1BhFO2e9vVJ"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"frequency\"></a>\n",
        "## üìî Creating a Frequency Dictionary"
      ]
    },
    {
      "metadata": {
        "id": "K-tbNHJ-9vVJ"
      },
      "cell_type": "markdown",
      "source": [
        "As our dataset is quite big, we'll only use those words that appear `k` times in our dataset. In this function, we'll create a frequency dictionary for our vocabulary."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Iz7uT10M9vVK"
      },
      "cell_type": "code",
      "source": [
        "def count_the_words(sentences) -> 'dict': # value ÿπÿØÿØ ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑŸÉŸÑŸÖÿ© , key  ÿßŸÑŸÉŸÑŸÖÿ©\n",
        "\n",
        "  # Creating a Dictionary of counts\n",
        "  word_counts = {}\n",
        "\n",
        "  # Iterating over sentences\n",
        "  for sentence in sentences:\n",
        "\n",
        "    # Iterating over Tokens\n",
        "    for token in sentence:\n",
        "\n",
        "      # Add count for new word\n",
        "      if token not in word_counts.keys():\n",
        "        word_counts[token] = 1\n",
        "\n",
        "      # Increase count by one\n",
        "      else:\n",
        "        word_counts[token] += 1\n",
        "\n",
        "  return word_counts"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_the_words(sentences) -> 'dict':\n",
        "\n",
        "  word_counts = {}\n",
        "  for sentence in sentences:\n",
        "\n",
        "    for token in sentence:\n",
        "\n",
        "      if token not in word_counts.keys():\n",
        "        word_counts[token] = 1\n",
        "\n",
        "      else:\n",
        "        word_counts[token] += 1\n",
        "\n",
        "  return word_counts"
      ],
      "metadata": {
        "id": "eaKNp0Xf99Df"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ELgmv9We9vVK"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"closed\"></a>\n",
        "## üîí Creating a Closed Vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "x8Go1G2V9vVL"
      },
      "cell_type": "markdown",
      "source": [
        "One of the most essential steps in dealing with Textual data is handling Out-of-vocabulary words. This helps the model to handle words which are not present in the training corpus. First step in this process is to create a `closed_vocabulary`. This function creates a closed vocabulary containing only those words according to the `count_threshold` parameter."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bwcY6yxM9vVL"
      },
      "cell_type": "code",
      "source": [
        "def handling_oov(tokenized_sentences, count_threshold) -> 'list': # oov means out of covab\n",
        "\n",
        "  # Empty list for closed vocabulary\n",
        "  closed_vocabulary = []\n",
        "\n",
        "  # Obtain frequency dictionary using previously defined function\n",
        "  words_count = count_the_words(tokenized_sentences)\n",
        "\n",
        "  # Iterate over words and counts\n",
        "  for word, count in words_count.items():\n",
        "\n",
        "    # Append if it's more(or equal) to the threshold\n",
        "    if count >= count_threshold :\n",
        "      closed_vocabulary.append(word)\n",
        "\n",
        "  return closed_vocabulary"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7QaPZ1oc9vVM"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"unk\"></a>\n",
        "## ü§∑üèª Adding UNK Tokens"
      ]
    },
    {
      "metadata": {
        "id": "ZZsKw_aB9vVN"
      },
      "cell_type": "markdown",
      "source": [
        "In this function we'll add `<unk>` tokens, to those words which are not in the `closed_vocabulary` which we just made."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7-A1Q0WV9vVN"
      },
      "cell_type": "code",
      "source": [
        "# ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸä ŸÖÿ® ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÅŸàŸÉÿßÿ®\n",
        "def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n",
        "\n",
        "  # Convert Vocabulary into a set\n",
        "  vocabulary = set(vocabulary)\n",
        "\n",
        "  # Create empty list for sentences\n",
        "  new_tokenized_sentences = []\n",
        "\n",
        "  # Iterate over sentences\n",
        "  for sentence in tokenized_sentences:\n",
        "\n",
        "    # Iterate over sentence and add <unk>\n",
        "    # if the token is absent from the vocabulary\n",
        "    new_sentence = []\n",
        "    for token in sentence:\n",
        "      if token in vocabulary:\n",
        "        new_sentence.append(token)\n",
        "      else:\n",
        "        new_sentence.append(unknown_token)\n",
        "\n",
        "    # Append sentece to the new list\n",
        "    new_tokenized_sentences.append(new_sentence)\n",
        "\n",
        "  return new_tokenized_sentences"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n",
        "\n",
        "#   vocabulary = set(vocabulary)\n",
        "#   new_tokenized_sentences = []\n",
        "\n",
        "#   for sentence in tokenized_sentences:\n",
        "\n",
        "#     new_sentence = []\n",
        "\n",
        "#     for token in sentence:\n",
        "#        if token in vocabulary:\n",
        "#         new_sentence.append(token)\n",
        "#        else:\n",
        "#         new_sentences.append(unkown_token)\n",
        "\n",
        "#     new_tokenized_sentences.append(new_sentence)\n",
        "\n",
        "#   return new_tokenized_sentences"
      ],
      "metadata": {
        "id": "gbc81fwx_jBF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cnA1WqE29vVO"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"final\"></a>\n",
        "## üßº Final Cleaning Pipeline"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "h5-bh7HV9vVO"
      },
      "cell_type": "code",
      "source": [
        "def cleansing(train_data, test_data, count_threshold):\n",
        "\n",
        "  # Get closed Vocabulary\n",
        "  vocabulary = handling_oov(train_data, count_threshold)\n",
        "\n",
        "  # Updated Training Dataset\n",
        "  new_train_data = unk_tokenize(train_data, vocabulary)\n",
        "\n",
        "  # Updated Test Dataset\n",
        "  new_test_data = unk_tokenize(test_data, vocabulary)\n",
        "\n",
        "  return new_train_data, new_test_data, vocabulary"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "90wf2O1E9vVP"
      },
      "cell_type": "code",
      "source": [
        "min_freq = 6\n",
        "final_train, final_test, vocabulary = cleansing(train, test, min_freq)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nl2T-LZp9vVQ"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"build\"></a>\n",
        "# üí™üèª Building The \"Model\""
      ]
    },
    {
      "metadata": {
        "id": "rcceOwqr9vVQ"
      },
      "cell_type": "markdown",
      "source": [
        "This is a helper function, which will come in handy during inference. This function returns a mapping from n-grams to their frequency in the dataset."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "C3EUaAyL9vVQ"
      },
      "cell_type": "code",
      "source": [
        "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
        "\n",
        "  # Empty dict for n-grams\n",
        "  n_grams = {}\n",
        "\n",
        "  # Iterate over all sentences in the dataset\n",
        "  for sentence in data:\n",
        "\n",
        "    # Append n start tokens and a single end token to the sentence\n",
        "    sentence = [start_token]*n + sentence + [end_token]\n",
        "\n",
        "    # Convert the sentence into a tuple\n",
        "    sentence = tuple(sentence)\n",
        "\n",
        "    # Temp var to store length from start of n-gram to end\n",
        "    m = len(sentence) if n==1 else len(sentence)-1\n",
        "\n",
        "    # Iterate over this length\n",
        "    for i in range(m):\n",
        "\n",
        "      # Get the n-gram\n",
        "      n_gram = sentence[i:i+n]\n",
        "\n",
        "      # Add the count of n-gram as value to our dictionary\n",
        "      # IF n-gram is already present\n",
        "      if n_gram in n_grams.keys():\n",
        "        n_grams[n_gram] += 1\n",
        "      # Add n-gram count\n",
        "      else:\n",
        "        n_grams[n_gram] = 1\n",
        "\n",
        "  return n_grams"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zEoawxT39vVR"
      },
      "cell_type": "markdown",
      "source": [
        "This function calculates the priority for the next word given the prior n-gram. This function also implements k-smoothing which helps account for unseen n-grams. Using the previously defined formula:\n",
        "\n",
        "\n",
        "$$\\large\n",
        "P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n)}{C(w_{n‚àíN+1:n‚àí1})}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### K-smoothing\n",
        "\n",
        "But what if we come across a n-gram that wasn't in the training set. Then our denominator would would become zero and our definition of probability will become invalid. Thus, we use k-smoothing, which adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary. This ensures any n-gram with zero count has the same probability of $\\frac{1}{|V|}$. Thus, our original estimation get's modified to:\n",
        "\n",
        "$$\\large\n",
        "P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n) + k}{C(w_{n‚àíN+1:n‚àí1} + k |V|)}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VgvLJzOu9vVR"
      },
      "cell_type": "code",
      "source": [
        "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
        "\n",
        "  # Convert the previous_n_gram into a tuple\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
        "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "\n",
        "  # The Denominator\n",
        "  denom = previous_n_gram_count + k * vocabulary_size\n",
        "\n",
        "  # previous n-gram plus the current word as a tuple\n",
        "  nplus1_gram = previous_n_gram + (word,)\n",
        "\n",
        "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero\n",
        "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
        "\n",
        "  # Numerator\n",
        "  num = nplus1_gram_count + k\n",
        "\n",
        "  # Final Fraction\n",
        "  prob = num / denom\n",
        "  return prob"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
        "\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "\n",
        "  denom = previous_n_gram_count + k * vocabulary_size\n",
        "\n",
        "\n",
        "  nplus1_gram = previous_n_gram + (word,)\n",
        "\n",
        "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
        "\n",
        "  num = nplus1_gram_count + k\n",
        "\n",
        "  prob = num / denom\n",
        "  return prob\n"
      ],
      "metadata": {
        "id": "KzvyB0CCCYPo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4W4ilE0N9vVT"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we loop over all the words in the vocabulary and then compute their probabilites using our `prob_for_single_word()` fn."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "sq3NqDxW9vVT"
      },
      "cell_type": "code",
      "source": [
        "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
        "\n",
        "  # Convert to Tuple\n",
        "  previous_n_gram = tuple(previous_n_gram)\n",
        "\n",
        "  # Add end and unknown tokens to the vocabulary\n",
        "  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "\n",
        "  # Calculate the size of the vocabulary\n",
        "  vocabulary_size = len(vocabulary)\n",
        "\n",
        "  # Empty dict for probabilites\n",
        "  probabilities = {}\n",
        "\n",
        "  # Iterate over words\n",
        "  for word in vocabulary:\n",
        "\n",
        "    # Calculate probability\n",
        "    probability = prob_for_single_word(word, previous_n_gram,\n",
        "                                           n_gram_counts, nplus1_gram_counts,\n",
        "                                           vocabulary_size, k=k)\n",
        "    # Create mapping: word -> probability\n",
        "    probabilities[word] = probability\n",
        "\n",
        "  return probabilities"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bMH-l-eB9vVT"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"auto-complete\"></a>\n",
        "# üí¨ The Auto-Complete System"
      ]
    },
    {
      "metadata": {
        "id": "nSmSNqzS9vVT"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we build our `auto_complete` fn. We simply loop over all the words in the vocabulary assuming that they can be the next word and then return the word with it's probability."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XXhIlZp09vVT"
      },
      "cell_type": "code",
      "source": [
        "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
        "\n",
        "\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "\n",
        "    # most recent 'n' words\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "\n",
        "    # Calculate probabilty for all words\n",
        "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
        "\n",
        "    # Intialize the suggestion and max probability\n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "\n",
        "    # Iterate over all words and probabilites, returning the max.\n",
        "    # We also add a check if the start_with parameter is provided\n",
        "    for word, prob in probabilities.items():\n",
        "\n",
        "        if start_with != None:\n",
        "\n",
        "            if not word.startswith(start_with):\n",
        "                continue\n",
        "\n",
        "        if prob > max_prob:\n",
        "\n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "\n",
        "    return suggestion, max_prob"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-pXFbjn39vVU"
      },
      "cell_type": "markdown",
      "source": [
        "We can also loop over all the various n-gram models to get multiple suggestions. This function just extends from the previously defined function by **taking multiple n-gram counts** instead of one. This allows us to take unigram, bigram, .. counts into account as well."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8WGAZLmL9vVU"
      },
      "cell_type": "code",
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "\n",
        "    # See how many models we have\n",
        "    count = len(n_gram_counts_list)\n",
        "\n",
        "    # Empty list for suggestions\n",
        "    suggestions = []\n",
        "\n",
        "    # IMP: Earlier \"-1\"\n",
        "\n",
        "    # Loop over counts\n",
        "    for i in range(count-1):\n",
        "\n",
        "        # get n and nplus1 counts\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
        "\n",
        "        # get suggestions\n",
        "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
        "                                    nplus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        # Append to list\n",
        "        suggestions.append(suggestion)\n",
        "\n",
        "    return suggestions"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "id": "msoP7Nox9vVV"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"inference\"></a>\n",
        "# üòä Inference"
      ]
    },
    {
      "metadata": {
        "id": "_PFIMQHf9vVV"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we create a list of n-gram counts for a arbitrary range `(1,6)`"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2g0Wy5H39vVW"
      },
      "cell_type": "code",
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    n_model_counts = count_n_grams(final_train, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1,6):\n",
        "  n_model_counts = count_n_grams(final_train, n)\n",
        "  n_gram_counts_list.append(n_model_counts)"
      ],
      "metadata": {
        "id": "REVu8B92GhDC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5JAqKAWZ9vVX"
      },
      "cell_type": "markdown",
      "source": [
        "Let's give it a sample input of \"i was about\" in a tokenized manner and get multiple suggestions using the above calculated n-gram counts with smoothing-factor, `k` = 1.0"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ZoABGLqj9vVY",
        "outputId": "04474479-82a1-4cf9-eb07-e70d678dbe1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "previous_tokens = [\"i\", \"was\", \"about\"]\n",
        "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "display(suggestion)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('the', 0.05352633604626057),\n",
              " ('to', 0.0028051829093754172),\n",
              " ('to', 0.0019167002089203228),\n",
              " ('lol', 1.92130341223486e-05)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previous_tokens = [\"i\", \"was\", \"about\"]\n",
        "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k= 1.0)\n",
        "\n",
        "display(suggestion)"
      ],
      "metadata": {
        "id": "7KHS0AScG1PV",
        "outputId": "b9be8bc9-8cdd-4963-9dd0-7bad0e52aec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('the', 0.05352633604626057),\n",
              " ('to', 0.0028051829093754172),\n",
              " ('to', 0.0019167002089203228),\n",
              " ('lol', 1.92130341223486e-05)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previous_tokens = [\"i\", \"am\", \"having\"]\n",
        "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "display(suggestion)"
      ],
      "metadata": {
        "id": "vQN9qFrkVB0g",
        "outputId": "0ccb52de-2b70-422e-bb55-0915d1b4a993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('a', 0.050603870671713944),\n",
              " ('a', 0.0007862238244995014),\n",
              " ('a', 0.0006138028925461312),\n",
              " ('lol', 1.92130341223486e-05)]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "3n-68U7-9vVc"
      },
      "cell_type": "markdown",
      "source": [
        "[Source](https://www.kaggle.com/code/sauravmaheshkar/auto-completion-using-n-gram-models) for this tutorial."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}